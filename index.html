<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Tiny Chat</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            -webkit-tap-highlight-color: transparent;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
            background: #000000;
            color: #ffffff;
            height: 100vh;
            overflow: hidden;
            -webkit-overflow-scrolling: touch;
            position: relative;
            line-height: 1.5;
        }
        
        /* Main App Content */
        .container {
            height: 100vh;
            display: flex;
            flex-direction: column;
            position: relative;
            overflow: hidden;
        }
        
        /* Header */
        .header {
            padding: 16px;
            text-align: center;
            font-size: 18px;
            font-weight: 500;
            color: #ffffff;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        /* Chat Area */
        .chat-area {
            flex: 1;
            overflow-y: auto;
            padding: 16px;
            display: flex;
            flex-direction: column;
            gap: 16px;
        }
        
        /* Messages */
        .message {
            max-width: 85%;
            padding: 14px 18px;
            border-radius: 18px;
            position: relative;
            line-height: 1.6;
            font-size: 16px;
            word-wrap: break-word;
        }
        
        .user-message {
            align-self: flex-end;
            background: #333333;
            color: #ffffff;
            border-bottom-right-radius: 6px;
        }
        
        .ai-message {
            align-self: flex-start;
            background: #222222;
            color: #ffffff;
            border-bottom-left-radius: 6px;
        }
        
        .system-message {
            align-self: center;
            background: #222222;
            color: #999999;
            max-width: 90%;
            font-style: italic;
            border-radius: 18px;
            font-size: 14px;
            padding: 10px 16px;
        }
        
        .error-message {
            background: #4a0000;
            color: #ff8888;
            border-radius: 18px;
            padding: 12px 16px;
            max-width: 90%;
            font-size: 15px;
            line-height: 1.5;
            border-left: 3px solid #ff4444;
        }
        
        .success-message {
            background: #1a3a1a;
            color: #a0e0a0;
            border-radius: 18px;
            padding: 12px 16px;
            max-width: 90%;
            font-size: 15px;
            line-height: 1.5;
            border-left: 3px solid #4CAF50;
        }
        
        .warning-message {
            background: #3a2a00;
            color: #ffcc00;
            border-radius: 18px;
            padding: 12px 16px;
            max-width: 90%;
            font-size: 15px;
            line-height: 1.5;
            border-left: 3px solid #ffcc00;
        }
        
        .loading-dots {
            display: inline-block;
            color: #999999;
        }
        
        .loading-dots::after {
            content: '.';
            animation: dots 1.5s infinite;
            width: 20px;
            display: inline-block;
            text-align: left;
        }
        
        @keyframes dots {
            0%, 20% { content: '.'; }
            40% { content: '..'; }
            60%, 100% { content: '...'; }
        }
        
        /* Input Area */
        .input-area {
            padding: 16px;
            border-top: 1px solid rgba(255, 255, 255, 0.1);
            background: #000000;
            position: relative;
        }
        
        .input-container {
            display: flex;
            gap: 12px;
            max-width: 100%;
        }
        
        .input-field {
            flex: 1;
            background: #111111;
            border: none;
            border-radius: 24px;
            padding: 12px 16px;
            color: #ffffff;
            font-family: inherit;
            font-size: 16px;
            line-height: 1.5;
            min-height: 44px;
            max-height: 200px;
            resize: none;
            outline: none;
            -webkit-appearance: none;
        }
        
        .input-field::placeholder {
            color: #666666;
        }
        
        .send-btn {
            background: #4361ee;
            color: white;
            border: none;
            border-radius: 24px;
            width: 48px;
            height: 48px;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            flex-shrink: 0;
            transition: all 0.2s ease;
        }
        
        .send-btn:hover {
            background: #3a56d4;
            transform: scale(1.05);
        }
        
        .send-btn:active {
            transform: scale(0.95);
        }
        
        .send-btn:disabled {
            background: #333333;
            cursor: not-allowed;
            transform: none;
        }
        
        .send-btn i {
            font-size: 18px;
        }
        
        /* Hide default scrollbar */
        .chat-area::-webkit-scrollbar {
            display: none;
        }
        
        .chat-area {
            -ms-overflow-style: none;
            scrollbar-width: none;
        }
        
        /* Debug info styling */
        .debug-info {
            background: #2a2a2a;
            border-radius: 12px;
            padding: 12px;
            margin-top: 12px;
            font-size: 13px;
            line-height: 1.4;
            color: #aaa;
        }
        
        .debug-info strong {
            color: #fff;
        }
        
        .debug-info code {
            background: #333;
            padding: 2px 6px;
            border-radius: 4px;
            color: #ffcc00;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">Pythia 160M Chat</div>
        
        <div class="chat-area" id="chat-box">
            <div class="system-message">
                <strong>System:</strong> Initializing AI agent. Downloading Pythia 160M (~190MB)...
            </div>
        </div>
        
        <div class="input-area">
            <div class="input-container">
                <textarea id="user-input" class="input-field" placeholder="Ask me anything..." disabled></textarea>
                <button id="send-btn" class="send-btn" disabled>
                    <i class="fas fa-arrow-up"></i>
                </button>
            </div>
        </div>
    </div>

    <script type="module">
        // ==================== DEBUG MODE ====================
        const DEBUG_MODE = true;
        
        // Log helper for debugging
        function logDebug(message, data = null) {
            if (DEBUG_MODE) {
                console.log(`[DEBUG] ${message}`, data ? data : '');
            }
        }
        
        // ==================== IMPORTS ====================
        logDebug('Loading Wllama library...');
        import { Wllama } from 'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.1.1/esm/index.js';

        // ==================== DOM ELEMENTS ====================
        const chatBox = document.getElementById('chat-box');
        const userInput = document.getElementById('user-input');
        const sendBtn = document.getElementById('send-btn');
        
        // ==================== STATE ====================
        let conversationHistory = [];
        let wllamaInstance = null;
        let isModelLoaded = false;
        
        // ==================== SYSTEM PROMPT ====================
        const SYSTEM_PROMPT = `You are a helpful AI assistant. Think carefully before answering. Be concise and informative.`;

        // ==================== CONFIGURATION ====================
        const CONFIG_PATHS = {
            'single-thread/wllama.wasm': 'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.1.1/esm/single-thread/wllama.wasm',
            'multi-thread/wllama.wasm' : 'https://cdn.jsdelivr.net/npm/@wllama/wllama@2.1.1/esm/multi-thread/wllama.wasm',
        };

        // iPhone optimizations
        const MODEL_CONFIG = {
            n_threads: 1,        // Single thread for iPhone compatibility
            n_ctx: 512,          // Smaller context = less memory
            seed: -1,            // Random seed
        };

        // ==================== MESSAGE FUNCTIONS ====================
        function addMessage(content, role, isLoading = false) {
            const messageEl = document.createElement('div');
            messageEl.classList.add('message');
            
            if (role === 'user') {
                messageEl.classList.add('user-message');
                messageEl.innerHTML = `<p>${escapeHtml(content)}</p>`;
            } else if (role === 'ai') {
                messageEl.classList.add('ai-message');
                if (isLoading) {
                    messageEl.innerHTML = `<div class="loading-dots">${content}</div>`;
                } else {
                    const paragraphs = content.split('\n').filter(p => p.trim());
                    messageEl.innerHTML = paragraphs.map(p => `<p>${escapeHtml(p.trim())}</p>`).join('');
                }
            } else if (role === 'system') {
                messageEl.classList.add('system-message');
                messageEl.innerHTML = content;
            } else if (role === 'error') {
                messageEl.classList.add('error-message');
                messageEl.innerHTML = content;
            } else if (role === 'success') {
                messageEl.classList.add('success-message');
                messageEl.innerHTML = content;
            } else if (role === 'warning') {
                messageEl.classList.add('warning-message');
                messageEl.innerHTML = content;
            }
            
            chatBox.appendChild(messageEl);
            chatBox.scrollTop = chatBox.scrollHeight;
            
            // Track conversation history
            if (role === 'user' || role === 'ai') {
                conversationHistory.push({ role, content });
                if (conversationHistory.length > 4) {
                    conversationHistory.shift();
                }
            }
            
            return messageEl;
        }
        
        // ==================== UTILITY FUNCTIONS ====================
        function escapeHtml(unsafe) {
            return unsafe
                .replace(/&/g, "&amp;")
                .replace(/</g, "&lt;")
                .replace(/>/g, "&gt;")
                .replace(/"/g, "&quot;")
                .replace(/'/g, "&#039;");
        }
        
        function isiPhone() {
            return /iPhone/.test(navigator.userAgent);
        }
        
        function getiOSVersion() {
            const match = navigator.userAgent.match(/OS (\d+)_(\d+)_?(\d+)?/);
            return match ? `${match[1]}.${match[2] || 0}.${match[3] || 0}` : 'Unknown';
        }
        
        function getBrowserInfo() {
            return {
                userAgent: navigator.userAgent,
                platform: navigator.platform,
                language: navigator.language,
                memory: navigator.deviceMemory || 'Unknown',
                hardwareConcurrency: navigator.hardwareConcurrency || 'Unknown'
            };
        }
        
        // ==================== RESPONSE CLEANING ====================
        function cleanModelResponse(text) {
            if (!text || typeof text !== 'string') {
                logDebug('Clean response: Invalid input');
                return "I need to think about that.";
            }
            
            // Remove special tokens
            let cleaned = text
                .replace(/<\|endoftext\|>/g, '')
                .replace(/<\|pad\|>/g, '')
                .replace(/<\|unk\|>/g, '')
                .replace(/<\/s>/g, '')  // Pythia stop token
                .replace(/<s>/g, '')
                .trim();
            
            logDebug('After token removal:', cleaned);
            
            // Truncate at problematic patterns
            const badPatterns = [
                /\n\s*\n\s*(?:Human|User|Question|Q:)/i,
                /(?:\. ){4,}/,
                /\.{4,}/
            ];
            
            for (const pattern of badPatterns) {
                const match = cleaned.match(pattern);
                if (match && match.index > 20) {
                    cleaned = cleaned.substring(0, match.index).trim();
                    logDebug('Truncated at pattern:', pattern);
                    break;
                }
            }
            
            // Remove excessive whitespace
            cleaned = cleaned
                .replace(/\s+/g, ' ')
                .replace(/\n{3,}/g, '\n\n')
                .trim();
            
            logDebug('After whitespace cleanup:', cleaned);
            
            // Quality checks
            if (cleaned.length < 5 || cleaned.split(' ').length < 2) {
                logDebug('Clean response: Failed quality check');
                return "I'm not certain. Could you rephrase your question?";
            }
            
            // Ensure proper sentence ending
            if (cleaned.length > 30 && !/[.!?]$/.test(cleaned)) {
                cleaned += '.';
            }
            
            logDebug('Final cleaned response:', cleaned);
            return cleaned;
        }
        
        // ==================== PROMPT BUILDING ====================
        function buildPrompt(userMessage) {
            logDebug('Building prompt for:', userMessage);
            
            // Simple format for Pythia (not chat-tuned)
            let prompt = `${SYSTEM_PROMPT}\n\n`;
            
            // Add recent conversation history
            const historyCount = Math.min(2, conversationHistory.length);
            for (let i = conversationHistory.length - historyCount; i < conversationHistory.length; i++) {
                const msg = conversationHistory[i];
                prompt += `${msg.role === 'user' ? 'Q' : 'A'}: ${msg.content}\n`;
            }
            
            prompt += `Q: ${userMessage}\nA:`;
            
            logDebug('Generated prompt:', prompt);
            return prompt;
        }
        
        // ==================== ERROR ANALYSIS ====================
        function analyzeError(error) {
            logDebug('Analyzing error:', error);
            
            const diag = {
                code: 'ERR_UNKNOWN',
                title: 'Unknown Error',
                description: 'An unexpected error occurred.',
                steps: [],
                details: []
            };
            
            diag.details.push(`Error name: ${error.name}`);
            diag.details.push(`Error message: ${error.message}`);
            diag.details.push(`Stack trace: ${error.stack ? error.stack.substring(0, 200) : 'N/A'}`);
            
            // CORS errors
            if (/CORS|cors|cross-origin|cross origin/i.test(error.message)) {
                diag.code = 'ERR_CORS';
                diag.title = 'CORS Error';
                diag.description = 'The server blocked the model download due to security restrictions.';
                diag.steps = [
                    'This is a server configuration issue, not your fault',
                    'The model host must enable CORS headers',
                    'Try a different model or hosting service'
                ];
            }
            // 404/403 errors
            else if (/404|403|not found|forbidden/i.test(error.message)) {
                diag.code = 'ERR_NOT_FOUND';
                diag.title = 'Model Not Found';
                diag.description = 'The model file could not be located at the specified URL.';
                diag.steps = [
                    'Verify the model repository exists on Hugging Face',
                    'Check the filename is correct',
                    'Try a different quantization (e.g., Q4_K_M)'
                ];
            }
            // Network errors
            else if (/network|fetch|failed to fetch|load/i.test(error.message.toLowerCase())) {
                diag.code = 'ERR_NETWORK';
                diag.title = 'Network Error';
                diag.description = 'Could not connect to download the model.';
                diag.steps = [
                    'Check your internet connection',
                    'Try using Wi-Fi instead of cellular data',
                    'Refresh the page and try again',
                    'Check if your network has restrictions'
                ];
            }
            // Memory errors
            else if (/memory|allocation|out of memory|heap/i.test(error.message.toLowerCase())) {
                diag.code = 'ERR_MEMORY';
                diag.title = 'Memory Error';
                diag.description = 'Your device ran out of memory while loading the model.';
                diag.steps = [
                    'Close all other browser tabs',
                    'Close background apps on your device',
                    'Try a smaller model (e.g., GPT-2 117M)',
                    'Restart your browser/device'
                ];
            }
            // WebAssembly errors
            else if (/wasm|WebAssembly|instantiate|compile/i.test(error.message)) {
                diag.code = 'ERR_WASM';
                diag.title = 'WebAssembly Error';
                diag.description = 'Your browser doesn\'t support required WebAssembly features.';
                diag.steps = [
                    'Update your browser to the latest version',
                    'Try Chrome or Firefox (better WASM support)',
                    'Enable WebAssembly in browser settings',
                    'Check for browser extensions blocking WASM'
                ];
            }
            // Timeout errors
            else if (/timeout|timed out/i.test(error.message.toLowerCase())) {
                diag.code = 'ERR_TIMEOUT';
                diag.title = 'Download Timeout';
                diag.description = 'The model download took too long and was canceled.';
                diag.steps = [
                    'Check your internet connection speed',
                    'Try downloading on a faster network',
                    'Be patient - large files take time on slow connections'
                ];
            }
            // Generic errors
            else {
                diag.steps = [
                    'Refresh the page and try again',
                    'Check your internet connection',
                    'Try a different browser',
                    'If the problem persists, the model may be unavailable'
                ];
            }
            
            logDebug('Error analysis complete:', diag);
            return diag;
        }
        
        // ==================== MODEL LOADING ====================
        async function loadModel() {
            logDebug('Starting model load process...');
            
            try {
                // Show device info for debugging
                if (isiPhone()) {
                    const iOSVersion = getiOSVersion();
                    addMessage(`üì± iPhone detected (iOS ${iOSVersion}). Using iPhone-optimized settings.`, "system");
                    addMessage("üí° Tip: Close other tabs/apps before loading for best performance.", "warning");
                }
                
                addMessage("Step 1: Initializing Wllama engine...", "system");
                
                // Initialize Wllama
                wllamaInstance = new Wllama(CONFIG_PATHS);
                logDebug('Wllama instance created');
                
                addMessage("Step 2: Downloading Pythia 160M model (~190MB)...", "system");
                
                // Load model with progress tracking
                const startTime = Date.now();
                let lastProgressUpdate = 0;
                
                await wllamaInstance.loadModelFromHF(
                    'TheBloke/pythia-160m-GGUF',       // Repository
                    'pythia-160m.Q4_K_M.gguf',          // Filename
                    {
                        ...MODEL_CONFIG,
                        progressCallback: ({ loaded, total }) => {
                            const progress = Math.round((loaded / total) * 100);
                            const now = Date.now();
                            
                            // Update every 10% or every 2 seconds
                            if (progress % 10 === 0 || progress === 100) {
                                if (progress !== lastProgressUpdate || now - lastProgressUpdate > 2000) {
                                    lastProgressUpdate = progress;
                                    
                                    const existing = chatBox.querySelector('.download-progress');
                                    const progressText = `Downloading model: ${progress}% (${Math.round(loaded / 1024 / 1024)}MB / ${Math.round(total / 1024 / 1024)}MB)`;
                                    
                                    if (existing) {
                                        existing.textContent = progressText;
                                    } else {
                                        const el = document.createElement('div');
                                        el.classList.add('system-message', 'download-progress');
                                        el.textContent = progressText;
                                        chatBox.appendChild(el);
                                        chatBox.scrollTop = chatBox.scrollHeight;
                                    }
                                    
                                    logDebug(`Download progress: ${progress}%`);
                                }
                            }
                        }
                    }
                );
                
                const loadTime = Math.round((Date.now() - startTime) / 1000);
                logDebug(`Model loaded in ${loadTime} seconds`);
                
                // Success!
                isModelLoaded = true;
                
                // Remove progress message
                const progressEl = chatBox.querySelector('.download-progress');
                if (progressEl) progressEl.remove();
                
                // Show success message with debug info
                const successHtml = `
                    <strong>‚úÖ Model Loaded Successfully!</strong>
                    <div class="debug-info">
                        <strong>Model:</strong> Pythia 160M Q4_K_M<br>
                        <strong>Size:</strong> ~190MB download ‚Üí ~220MB in memory<br>
                        <strong>Load time:</strong> ${loadTime} seconds<br>
                        <strong>Device:</strong> ${isiPhone() ? `iPhone (iOS ${getiOSVersion()})` : navigator.platform}<br>
                        <strong>Threads:</strong> ${MODEL_CONFIG.n_threads}<br>
                        <strong>Context size:</strong> ${MODEL_CONFIG.n_ctx}
                    </div>
                `;
                addMessage(successHtml, "success");
                
                addMessage("I'm ready! Ask me anything ‚Äî I'm running Pythia 160M, optimized for your device.", "ai");
                addMessage("üí° Keep questions clear and specific for best results.", "system");
                
                // Enable UI
                userInput.disabled = false;
                sendBtn.disabled = false;
                userInput.focus();
                
                logDebug('Model loading complete. UI enabled.');
                
            } catch (error) {
                logDebug('Model loading failed:', error);
                
                // Remove progress message
                const progressEl = chatBox.querySelector('.download-progress');
                if (progressEl) progressEl.remove();
                
                // Analyze and display error
                const diag = analyzeError(error);
                
                const errorHtml = `
                    <strong>‚ùå Model Loading Failed</strong>
                    <div class="debug-info">
                        <strong>Diagnostic Code:</strong> <code>${diag.code}</code><br>
                        <strong>Error:</strong> ${diag.title}<br>
                        <strong>Details:</strong> ${diag.description}
                    </div>
                    
                    <div class="debug-info">
                        <strong>Troubleshooting Steps:</strong>
                        <ul style="margin-top: 8px; padding-left: 20px;">
                            ${diag.steps.map(step => `<li>${step}</li>`).join('')}
                        </ul>
                    </div>
                    
                    <div class="debug-info">
                        <strong>Technical Details:</strong>
                        <ul style="margin-top: 8px; padding-left: 20px;">
                            ${diag.details.map(detail => `<li>${detail}</li>`).join('')}
                        </ul>
                    </div>
                `;
                
                addMessage(errorHtml, "error");
                
                // Offer fallback option
                addMessage("üîÑ <strong>Fallback Option:</strong> If Pythia 160M doesn't work, try GPT-2 117M (smaller, more compatible):", "warning");
                
                const fallbackBtn = document.createElement('button');
                fallbackBtn.textContent = "Switch to GPT-2 117M";
                fallbackBtn.style.cssText = `
                    background: #4361ee;
                    color: white;
                    border: none;
                    border-radius: 12px;
                    padding: 8px 16px;
                    margin-top: 8px;
                    cursor: pointer;
                    font-size: 14px;
                `;
                fallbackBtn.onclick = () => {
                    fallbackBtn.disabled = true;
                    fallbackBtn.textContent = "Loading...";
                    loadGPT2Fallback();
                };
                
                const fallbackContainer = document.createElement('div');
                fallbackContainer.style.cssText = 'text-align: center; margin-top: 10px;';
                fallbackContainer.appendChild(fallbackBtn);
                chatBox.appendChild(fallbackContainer);
                chatBox.scrollTop = chatBox.scrollHeight;
            }
        }
        
        // ==================== FALLBACK: GPT-2 117M ====================
        async function loadGPT2Fallback() {
            logDebug('Loading GPT-2 117M fallback...');
            
            try {
                addMessage("üîÑ Loading GPT-2 117M fallback model (~113MB)...", "system");
                
                // Reinitialize Wllama
                wllamaInstance = new Wllama(CONFIG_PATHS);
                
                await wllamaInstance.loadModelFromHF(
                    'RichardErkhov/Arjun-G-Ravi_-_chat-GPT2-gguf',
                    'chat-GPT2.Q4_K.gguf',
                    {
                        ...MODEL_CONFIG,
                        n_ctx: 1024,  // GPT-2 can handle larger context
                        progressCallback: ({ loaded, total }) => {
                            const progress = Math.round((loaded / total) * 100);
                            if (progress % 20 === 0 || progress === 100) {
                                const existing = chatBox.querySelector('.download-progress');
                                const progressText = `Downloading GPT-2: ${progress}%`;
                                if (existing) {
                                    existing.textContent = progressText;
                                } else {
                                    const el = document.createElement('div');
                                    el.classList.add('system-message', 'download-progress');
                                    el.textContent = progressText;
                                    chatBox.appendChild(el);
                                    chatBox.scrollTop = chatBox.scrollHeight;
                                }
                            }
                        }
                    }
                );
                
                isModelLoaded = true;
                
                // Remove progress message
                const progressEl = chatBox.querySelector('.download-progress');
                if (progressEl) progressEl.remove();
                
                addMessage("‚úÖ GPT-2 117M loaded successfully! This model is smaller and more compatible with older devices.", "success");
                addMessage("I'm ready! Ask me anything.", "ai");
                
                userInput.disabled = false;
                sendBtn.disabled = false;
                userInput.focus();
                
            } catch (error) {
                logDebug('GPT-2 fallback failed:', error);
                addMessage(`‚ùå GPT-2 fallback also failed: ${error.message}`, "error");
            }
        }
        
        // ==================== CHAT HANDLING ====================
        async function handleChat() {
            const text = userInput.value.trim();
            if (!text) return;
            
            logDebug('Handling chat message:', text);
            
            // Add user message
            addMessage(text, 'user');
            userInput.value = '';
            userInput.style.height = 'auto';
            
            // Show loading
            const loadingEl = addMessage("Thinking...", 'ai', true);
            sendBtn.disabled = true;
            userInput.disabled = true;
            
            try {
                if (!isModelLoaded || !wllamaInstance) {
                    throw new Error('Model not loaded. Please wait for initialization to complete.');
                }
                
                const prompt = buildPrompt(text);
                logDebug('Generated prompt length:', prompt.length);
                
                // Generate response
                const response = await wllamaInstance.createCompletion(prompt, {
                    nPredict: 120,  // Limit response length for memory
                    sampling: { 
                        temp: 0.7,
                        top_k: 40,
                        top_p: 0.9,
                        repeat_penalty: 1.2,
                        frequency_penalty: 0.3
                    }
                });
                
                logDebug('Raw model response:', response);
                
                // Clean response
                let cleanedResponse = cleanModelResponse(response);
                
                // Add knowledge cutoff disclaimer for time-sensitive questions
                const lowerQ = text.toLowerCase();
                if (/\b202[3456789]\b|\btoday\b|\bcurrent\b|\blatest\b|\bnews\b|\btrending\b/.test(lowerQ) && 
                    !/\bhistory\b|\bpast\b|\bhistorical\b/.test(lowerQ)) {
                    if (!cleanedResponse.toLowerCase().includes("cutoff") && 
                        !cleanedResponse.toLowerCase().includes("training")) {
                        cleanedResponse += " (Note: My knowledge has a cutoff and I can't access real-time information.)";
                    }
                }
                
                logDebug('Cleaned response:', cleanedResponse);
                
                // Replace loading with response
                loadingEl.remove();
                addMessage(cleanedResponse, 'ai');
                
            } catch (error) {
                logDebug('Chat generation error:', error);
                
                loadingEl.remove();
                
                if (error.message.includes('memory') || error.message.includes('allocation')) {
                    addMessage("‚ö†Ô∏è Your device is low on memory. Try:\n‚Ä¢ Shorter questions\n‚Ä¢ Close other tabs/apps\n‚Ä¢ Wait a few seconds between messages", "warning");
                } else if (error.message.includes('Model not loaded')) {
                    addMessage("‚è≥ The model is still loading. Please wait a moment and try again.", "warning");
                } else {
                    addMessage(`‚ö†Ô∏è ${error.message || 'I had trouble generating a response. Try a simpler question.'}`, "ai");
                }
            } finally {
                sendBtn.disabled = false;
                userInput.disabled = false;
                userInput.focus();
            }
        }
        
        // ==================== EVENT LISTENERS ====================
        sendBtn.addEventListener('click', handleChat);
        
        userInput.addEventListener('keypress', (e) => { 
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                handleChat();
            }
        });
        
        userInput.addEventListener('input', () => {
            userInput.style.height = 'auto';
            userInput.style.height = Math.min(200, userInput.scrollHeight) + 'px';
        });
        
        // ==================== INITIALIZATION ====================
        async function init() {
            logDebug('=== INITIALIZATION STARTED ===');
            logDebug('Browser info:', getBrowserInfo());
            logDebug('Is iPhone:', isiPhone());
            
            // Show initialization message
            addMessage("üöÄ Starting AI agent initialization...", "system");
            
            // Start model loading
            await loadModel();
            
            logDebug('=== INITIALIZATION COMPLETE ===');
        }
        
        // Start the app
        init().catch(err => {
            logDebug('Initialization error:', err);
            addMessage(`‚ùå Critical error: ${err.message}`, "error");
        });
    </script>
</body>
</html>

